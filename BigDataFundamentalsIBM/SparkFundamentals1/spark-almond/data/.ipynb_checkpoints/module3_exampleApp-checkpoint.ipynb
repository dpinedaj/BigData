{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "// import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` // Or use any other 2.x version here\n",
    "// import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfunc\u001b[39m: (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m) => \u001b[32mString\u001b[39m = ammonite.$sess.cmd1$Helper$$Lambda$2690/134134422@1c564033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val func = (x: String, y: String) => x + \" \" + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"hola mundo\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(\"hola\", \"mundo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mHdfsTest\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object HdfsTest {\n",
    "\n",
    "    def main(args: Array[String]) {\n",
    "        if (args.length < 1) {\n",
    "            System.err.println(\"Usage: HdfsTest <file>\")\n",
    "            System.exit(1)\n",
    "        }\n",
    "        val sc: SparkContext = new SparkContext(\"local[1]\", \"Examples\")\n",
    "        try {\n",
    "        val file = sc.textFile(args(0))\n",
    "        val mapped = file.map(s => s.length).cache()\n",
    "        for (iter <- 1 to 10) {\n",
    "            val start = System.currentTimeMillis()\n",
    "            for (x <- mapped) { x + 2 }\n",
    "            val end = System.currentTimeMillis()\n",
    "            println(\"Iteration \" + iter + \"took\" + (end-start) + \" ms\")\n",
    "        }\n",
    "        } finally {\n",
    "        sc.stop()\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1took443 ms\n",
      "Iteration 2took31 ms\n",
      "Iteration 3took30 ms\n",
      "Iteration 4took28 ms\n",
      "Iteration 5took34 ms\n",
      "Iteration 6took30 ms\n",
      "Iteration 7took27 ms\n",
      "Iteration 8took26 ms\n",
      "Iteration 9took27 ms\n",
      "Iteration 10took27 ms\n"
     ]
    }
   ],
   "source": [
    "HdfsTest [\"untitled.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mHdfsTest\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object HdfsTest {\n",
    "\n",
    "    def run(args: Array[String]) {\n",
    "        if (args.length < 1) {\n",
    "            System.err.println(\"Usage: HdfsTest <file>\")\n",
    "            System.exit(1)\n",
    "        }\n",
    "        val sc: SparkContext = new SparkContext(\"local[1]\", \"Examples\")\n",
    "        try {\n",
    "        val file = sc.textFile(args(0))\n",
    "        val mapped = file.map(s => s.length)\n",
    "        for (iter <- 1 to 10) {\n",
    "            val start = System.currentTimeMillis()\n",
    "            for (x <- mapped) { x + 2 }\n",
    "            val end = System.currentTimeMillis()\n",
    "            println(\"Iteration \" + iter + \"took\" + (end-start) + \" ms\")\n",
    "        }\n",
    "        } finally {\n",
    "        sc.stop()\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1took70 ms\n",
      "Iteration 2took33 ms\n",
      "Iteration 3took30 ms\n",
      "Iteration 4took32 ms\n",
      "Iteration 5took30 ms\n",
      "Iteration 6took40 ms\n",
      "Iteration 7took30 ms\n",
      "Iteration 8took34 ms\n",
      "Iteration 9took27 ms\n",
      "Iteration 10took27 ms\n"
     ]
    }
   ],
   "source": [
    "HdfsTest.run(Array(\"untitled.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
